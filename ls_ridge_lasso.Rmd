---
title: "LS, Lasso, Ridge Regression"
author: "Adina Zhang"
date: "April 4, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(caret)
library(glmnet)
```

```{r, message = FALSE}
# Load dataset
life_analysis = read_csv("./Life Expectancy Data.csv") %>% 
  janitor::clean_names() %>%
  select(-country, -population, -infant_deaths, -measles, -polio,
         -percentage_expenditure) %>% 
  mutate(log_gdp = log(gdp + 0.1),
         log_under_five = log(under_five_deaths + 0.1),
         status = factor(status),
         status = fct_recode(status, "0" = "Developing", "1" = "Developed"),
         status = ifelse(status == "1", 1, 0)) %>% 
  select(-gdp, -under_five_deaths)

life_analysis = na.omit(life_analysis)

# Split data into test and training datasets
set.seed(1)
train_rows = createDataPartition(life_analysis$life_expectancy,
                                 p = 0.75,
                                 list = F)

# Specify test and training datasets
train_df = life_analysis[train_rows,]
# Test Set
test_df = life_analysis[-train_rows,]

# Predictor variable
x = model.matrix(life_expectancy~., train_df)[,-3]
x2 = model.matrix(life_expectancy~., test_df)[,-3]
# Outcome variable
y = train_df$life_expectancy
y2 = test_df$life_expectancy
```


## Least Squares Model

```{r}
# Set up cross validation measures
ctrl1 = trainControl(method = "cv", number = 10)

set.seed(2)
# Least squares model
ls.fit = train(life_expectancy ~.,
               data = train_df,
               method = 'lm',
               trControl = ctrl1)
# MSE Calculation
predy.lm = predict(ls.fit$finalModel, newdata = test_df)
mse = mean((predy.lm - test_df$life_expectancy)^2)
```

## Ridge Regression

```{r}
set.seed(123)
# Ridge model
ridge.fit = train(x, y,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = 0,
                                         lambda = exp(seq(-2, 20, length = 100))),
                  trControl = ctrl1)
best.lambda = ridge.fit$bestTune$lambda

# MSE Calculation
predy.ridge = predict(ridge.fit$finalModel, 
                      newx = x2,
                      s = best.lambda, type = "response")
mse_ridge = mean((predy.ridge - y2)^2)

plot(ridge.fit, xTrans = function(x) log(x))
```

### Lasso

```{r}
set.seed(123)
# Ridge model
lasso.fit = train(x, y,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = 1,
                                         lambda = exp(seq(-2, 20, length = 100))),
                  trControl = ctrl1)
best.lambda = lasso.fit$bestTune$lambda

# MSE Calculation
predy.lasso = predict(lasso.fit$finalModel, 
                      newx = x2,
                      s = best.lambda, type = "response")
mse_lasso = mean((predy.lasso - y2)^2)

plot(lasso.fit, xTrans = function(x) log(x))

# Summary of coefficients
coefficients = predict(lasso.fit$finalModel, newx = x2,
                       s = best.lambda, type = "coefficients")
summary(coefficients)
```


```{r}
set.seed(2)

```

